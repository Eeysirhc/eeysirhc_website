---
title: Simulating data to explore page speed performance
author: Christopher Yee
date: '2019-09-23'
slug: exploring-page-speed-with-data-simulation
categories:
  - Data Science
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>
<link href="/rmarkdown-libs/lightable/lightable.css" rel="stylesheet" />


<p>We may be inundated with data but sometimes collecting it can be a challenge in and of itself. A few reasons off the top of my head:</p>
<ul>
<li>Sparsity</li>
<li>Difficult to measure</li>
<li>Impractical to devote company resources to it</li>
<li>Lack of technical expertise to actually build or acquire it</li>
<li>Lazy (yours truly - <a href="https://www.christopheryee.org/blog/moz-put-your-money-where-your-diversity-mouth-is/">except for that one time</a>)</li>
</ul>
<p>Through simulation we can generate our own dataset with the added benefit of fully understanding what features we choose to put in our models (or leave out).</p>
<p>In fact, a few of the machine learning models I wrote and put into production <a href="https://www.ftoptimize.com">at work</a> are based on simulated data!</p>
<p>This article will provide a quick walkthrough in getting you up and running using <a href="https://twitter.com/search?q=%23rstats">#rstats</a>.</p>
<div id="background" class="section level2">
<h2>Background</h2>
<p>I am in the market for a smart camera so while shopping online I also compiled some page speed data for a few eCommerce websites. You can follow along with the data here:</p>
<pre class="r"><code>library(tidyverse)
library(scales)
library(knitr)
library(kableExtra)

df &lt;- read_csv(&quot;https://raw.githubusercontent.com/Eeysirhc/random_datasets/master/page_speed_benchmark.csv&quot;)

# VIEW RANDOM SAMPLE
df_sample &lt;- df %&gt;% 
  sample_n(10) </code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
website
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
page_type
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
product
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
time_to_interactive
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Electrical
</td>
<td style="text-align:right;">
9.8
</td>
</tr>
<tr>
<td style="text-align:left;">
Home Depot
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
25.1
</td>
</tr>
<tr>
<td style="text-align:left;">
IKEA
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Entertainment
</td>
<td style="text-align:right;">
9.8
</td>
</tr>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
9.8
</td>
</tr>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Electrical
</td>
<td style="text-align:right;">
8.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
7.1
</td>
</tr>
<tr>
<td style="text-align:left;">
Walmart
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
15.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:left;">
Home
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:right;">
10.4
</td>
</tr>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
7.1
</td>
</tr>
<tr>
<td style="text-align:left;">
BestBuy
</td>
<td style="text-align:left;">
Category
</td>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
29.4
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>I used Google Chrome’s built-in page audit (<a href="https://developers.google.com/web/tools/lighthouse">Lighthouse</a>) to log the time for each website, page type and product category.</p>
<p>There are other page speed metrics but for educational purposes we’ll just focus on using <em>time_to_interactive</em>.</p>
<p>Lets pose the question: <em>which site is fastest in terms of time_to_interactive?</em></p>
</div>
<div id="standard-approach" class="section level2">
<h2>Standard approach</h2>
<p>One way to answer that question is to create descriptive statistics by computing the averages, finding the percent difference from the fastest site and then calling it a day.</p>
<pre class="r"><code>df_standard &lt;- df %&gt;% 
  filter(page_type != &#39;Home&#39;) %&gt;% 
  group_by(website) %&gt;% 
  summarize(time_interactive = mean(time_to_interactive)) %&gt;% 
  ungroup() %&gt;% 
  arrange(time_interactive) %&gt;% 
  mutate(slower_than_amazon = round((time_interactive / 7.59 - 1) * 100, 0),
         slower_than_amazon = paste0(slower_than_amazon, &quot;%&quot;)) </code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
website
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
time_interactive
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
slower_than_amazon
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:right;">
7.59000
</td>
<td style="text-align:left;">
0%
</td>
</tr>
<tr>
<td style="text-align:left;">
IKEA
</td>
<td style="text-align:right;">
9.44000
</td>
<td style="text-align:left;">
24%
</td>
</tr>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:right;">
9.44375
</td>
<td style="text-align:left;">
24%
</td>
</tr>
<tr>
<td style="text-align:left;">
Walmart
</td>
<td style="text-align:right;">
16.63077
</td>
<td style="text-align:left;">
119%
</td>
</tr>
<tr>
<td style="text-align:left;">
Target
</td>
<td style="text-align:right;">
21.36667
</td>
<td style="text-align:left;">
182%
</td>
</tr>
<tr>
<td style="text-align:left;">
Home Depot
</td>
<td style="text-align:right;">
25.20000
</td>
<td style="text-align:left;">
232%
</td>
</tr>
<tr>
<td style="text-align:left;">
BestBuy
</td>
<td style="text-align:right;">
31.23077
</td>
<td style="text-align:left;">
311%
</td>
</tr>
</tbody>
</table>
<p>However, there is a problem - sample size!</p>
<pre class="r"><code>df_size &lt;- df %&gt;% 
  filter(page_type != &#39;Home&#39;) %&gt;% 
  group_by(website) %&gt;% 
  summarize(time_interactive = mean(time_to_interactive),
            count = n()) %&gt;% 
  ungroup() %&gt;% 
  arrange(desc(count))</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
website
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
time_interactive
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
count
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:right;">
9.44375
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:left;">
BestBuy
</td>
<td style="text-align:right;">
31.23077
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
Walmart
</td>
<td style="text-align:right;">
16.63077
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:right;">
7.59000
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
Home Depot
</td>
<td style="text-align:right;">
25.20000
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
Target
</td>
<td style="text-align:right;">
21.36667
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
IKEA
</td>
<td style="text-align:right;">
9.44000
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>eCommerce sites have hundreds of thousands of pages so how can we be so certain our summary captures actual page speed performance? Perhaps adding in a confidence interval?</p>
<p>At a more granular level, IKEA and Smarthome have exactly the same average <em>time_to_interactive</em> of 9.44 seconds but I recorded fewer samples with the former - which site is actually faster?</p>
<pre class="r"><code>df %&gt;% 
  filter(website %in% c(&#39;IKEA&#39;, &#39;Smarthome&#39;)) %&gt;% 
  ggplot(aes(website, time_to_interactive, color = website)) +
  geom_point(show.legend = FALSE, size = 5, alpha = 0.5) +
  geom_hline(yintercept = 9.44, lty = 2, color = &#39;red&#39;) +
  scale_color_manual(values = cbPalette3) +
  scale_y_continuous(limits = c(0, 15)) +
  labs(x = NULL, y = &quot;Time to Interactive (seconds)&quot;,
       subtitle = &quot;Dashed line represents average of 9.44s&quot;)</code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>What if I don’t know how to write a script to grab every URL and then feed it into Google Lighthouse? Or more realistically, what if I am not inclined to go and collect 11 more data points for IKEA?</p>
<p>These questions can be answered with the help of simulation. By applying the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a> and <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a> we can directly address measurement uncertainty.</p>
</div>
<div id="simulating-big-data" class="section level2">
<h2>Simulating “big” data</h2>
<p>To get started we will need three values:</p>
<ul>
<li><em>n</em> = number of observations</li>
<li><em>mean</em> = vector of means</li>
<li><em>sd</em> = vector of standard deviations</li>
</ul>
<pre class="r"><code>df_summary &lt;- df %&gt;% 
  filter(page_type != &#39;Home&#39;) %&gt;% 
  group_by(website) %&gt;% 
  summarize(time_interactive = mean(time_to_interactive),
            sd = sd(time_to_interactive),
            count = n()) %&gt;% 
  ungroup() %&gt;% 
  arrange(time_interactive)</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
website
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
time_interactive
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
sd
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
count
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Amazon
</td>
<td style="text-align:right;">
7.59000
</td>
<td style="text-align:right;">
1.9081405
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
IKEA
</td>
<td style="text-align:right;">
9.44000
</td>
<td style="text-align:right;">
0.6877500
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
Smarthome
</td>
<td style="text-align:right;">
9.44375
</td>
<td style="text-align:right;">
1.1488944
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:left;">
Walmart
</td>
<td style="text-align:right;">
16.63077
</td>
<td style="text-align:right;">
0.8300448
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
Target
</td>
<td style="text-align:right;">
21.36667
</td>
<td style="text-align:right;">
2.0175893
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
Home Depot
</td>
<td style="text-align:right;">
25.20000
</td>
<td style="text-align:right;">
1.4055446
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
BestBuy
</td>
<td style="text-align:right;">
31.23077
</td>
<td style="text-align:right;">
2.6042864
</td>
<td style="text-align:right;">
13
</td>
</tr>
</tbody>
</table>
<p>Now that we have the minimum requirements we can simulate our data. Let’s start with IKEA:</p>
<pre class="r"><code>ikea &lt;- rnorm(1e4, 9.44, 0.688) %&gt;% 
  as_tibble() %&gt;% 
  mutate(website = paste0(&quot;ikea&quot;))</code></pre>
<p>There was a lot to unpack there so let’s break it down:</p>
<ul>
<li><strong>rnorm</strong> is the R function to generate random numbers from a <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a></li>
<li>1e4 is scientific notation for 10,000 observations</li>
<li>9.44 is the average mean for IKEA <em>time_to_interactive</em></li>
<li>0.688 is the standard deviation</li>
<li>We moved our data into the tidyverse with <em>as_tibble()</em></li>
<li>Used the <strong>mutate</strong> function to add a <em>website</em> column and identify IKEA for the set of results</li>
</ul>
<p>We can now plot our distribution of <em>time_to_interactive</em> scores for IKEA where we generated 10K data points.</p>
<pre class="r"><code>ikea %&gt;% 
  ggplot(aes(value, fill = website)) +
  geom_histogram(position = &#39;identity&#39;, binwidth = 0.05, alpha = 0.8,
                 show.legend = FALSE) +
  scale_fill_manual(values = cbPalette) +
  scale_x_continuous(limits = c(0, 20)) +
  labs(x = &quot;Time to Interactive (seconds)&quot;, y = NULL)</code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>What this illustrates is the potential frequency of page speed scores and the range of all possible values.</p>
<p>In other words, if we take a random page on IKEA and measure its <em>time_to_interactive</em> we know scores could be as low as 8s or as high as 11s. Additionally, there is a central tendency for scores to fall around 9s but it is not at all possible to have a score of 1s or more than 13s.</p>
<p>This is an improvement from a single summary statistic but what if we wanted to ask more complicated questions? What if we wanted to know the likelihood a random IKEA page was greater than 11s? Or which site is faster: Amazon vs IKEA?</p>
<p>The solution is to condition on our simulated dataset.</p>
</div>
<div id="conditioning-on-the-imaginary" class="section level2">
<h2>Conditioning on the imaginary</h2>
<p>Lets ask the question: <em>what is the probability IKEA will have a page speed greater than 11 seconds?</em></p>
<p>We can easily get that answer by sampling from our data:</p>
<pre class="r"><code>sum(ikea$value &gt; 11) / length(ikea$value) * 100</code></pre>
<pre><code>## [1] 1.45</code></pre>
<p>And to drive that home with some data viz…</p>
<pre class="r"><code>ikea %&gt;% 
  mutate(greater_11s = ifelse(value &gt; 11, &#39;yes&#39;, &#39;no&#39;)) %&gt;% 
  ggplot(aes(value, fill = greater_11s)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  scale_fill_manual(values = cbPalette) +
  scale_x_continuous(limits = c(0, 20)) +
  labs(x = &quot;Time to Interactive (seconds)&quot;, y = NULL)  </code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We can also head in the opposite direction: <em>what is the probability IKEA will have a page speed of less than 9 seconds?</em></p>
<pre class="r"><code>sum(ikea$value &lt; 9) /length(ikea$value) * 100</code></pre>
<pre><code>## [1] 25.72</code></pre>
<p>And to plot our results…</p>
<pre class="r"><code>ikea %&gt;% 
  mutate(less_than_9s = ifelse(value &lt; 9, &#39;yes&#39;, &#39;no&#39;)) %&gt;% 
  ggplot(aes(value, fill = less_than_9s)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  scale_fill_manual(values = cbPalette) +
  scale_x_continuous(limits = c(0, 20)) +
  labs(x = &quot;Time to Interactive (seconds)&quot;, y = NULL)  </code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We can also ask more complicated quesitons such as: <em>what is the probability a random Amazon page will be faster than IKEA?</em></p>
<pre class="r"><code># SIMULATE AMAZON DATA
amazon &lt;- rnorm(1e4, 7.59, 1.91) %&gt;%
  as_tibble() %&gt;% 
  mutate(website = paste0(&quot;amazon&quot;))

mean(amazon$value &lt; ikea$value) * 100</code></pre>
<pre><code>## [1] 82.39</code></pre>
<p>And…well…you get the idea….</p>
<pre class="r"><code>pagespeed &lt;- rbind(ikea, amazon)

pagespeed %&gt;% 
  ggplot(aes(value, fill = website)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7, position = &#39;identity&#39;) +
  scale_fill_manual(values = cbPalette2) +
  scale_x_continuous(limits = c(0, 20)) +
  labs(x = &quot;Time to Interactive (seconds)&quot;, y = NULL) </code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div id="side-note" class="section level3">
<h3>Side note</h3>
<p>From my personal experience working at large companies, business executives respond very well to probabilities. Thus, <em>“our site is 24% slower than Amazon”</em> is not as impactful as stating <em>“if we were to take 10K random pages from each site, there is an 82% chance our site will be slower than Amazon.”</em></p>
</div>
</div>
<div id="what-about-amazon" class="section level2">
<h2>What about Amazon?</h2>
<p>In the past I wrote about <a href="https://www.christopheryee.org/blog/for-the-love-of-data-segment/">segmenting data</a> to reveal deeper insights hidden beneath the aggregation.</p>
<p>So, what is <em>really</em> driving Amazon’s page speed score higher? Lets simulate some data and for fun we’ll increase our observations from 1e4 (10K) to 1e5 (100K). Quick reminder about Amazon page speed data:</p>
<pre class="r"><code>df_amazon &lt;- df %&gt;% 
  filter(website == &#39;Amazon&#39;, page_type != &#39;Home&#39;) %&gt;% 
  group_by(product) %&gt;% 
  summarize(time_interactive = mean(time_to_interactive),
            sd = sd(time_to_interactive)) %&gt;% 
  arrange(time_interactive)</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
product
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
time_interactive
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;font-weight: bold;">
sd
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Electrical
</td>
<td style="text-align:right;">
5.750000
</td>
<td style="text-align:right;">
0.212132
</td>
</tr>
<tr>
<td style="text-align:left;">
Security
</td>
<td style="text-align:right;">
7.933333
</td>
<td style="text-align:right;">
2.458319
</td>
</tr>
<tr>
<td style="text-align:left;">
Sensors
</td>
<td style="text-align:right;">
8.066667
</td>
<td style="text-align:right;">
1.674316
</td>
</tr>
<tr>
<td style="text-align:left;">
Entertainment
</td>
<td style="text-align:right;">
8.200000
</td>
<td style="text-align:right;">
2.545584
</td>
</tr>
</tbody>
</table>
<p>And the code to generate our data with the subsequent plot:</p>
<pre class="r"><code>electrical &lt;- rnorm(1e5, 5.75, 0.212) %&gt;% as_tibble() %&gt;% 
  mutate(product = paste0(&quot;electrical&quot;))

security &lt;- rnorm(1e5, 7.93, 2.46) %&gt;% as_tibble() %&gt;% 
  mutate(product = paste0(&quot;security&quot;))

sensors &lt;- rnorm(1e5, 8.07, 1.67) %&gt;% as_tibble() %&gt;% 
  mutate(product = paste0(&quot;sensors&quot;))

entertainment &lt;- rnorm(1e5, 8.2, 2.55) %&gt;% as_tibble() %&gt;% 
  mutate(product = paste0(&quot;entertainment&quot;))

amazon &lt;- rbind(electrical, security, sensors, entertainment)

amazon %&gt;% 
  ggplot(aes(value, fill = product)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7, position = &#39;identity&#39;) +
  scale_x_continuous(limits = c(0, 20)) +
  scale_y_continuous(labels = comma_format()) +
  scale_fill_brewer(palette = &#39;Spectral&#39;) +
  labs(x = &quot;Time to Interactive (seconds)&quot;, y = NULL,
       title = &quot;Amazon: time to interactive by product category (n=100K)&quot;)</code></pre>
<p><img src="/blog/2019-09-19-exploring-page-speed-with-data-simulation_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Well, this is quite shocking (pun intended) - the electrical category’s <em>time_to_interactive</em> is not only faster but its central tendency is also less dispersed than the other three.</p>
<p>Why might that be the case? Specific business focus on these products? Less teams working on it? Not enough products? All conjecture on my part without digging deeper into the site.</p>
<p>Finally, what is the probability the electrical category is faster than security (2nd place)?</p>
<pre class="r"><code>mean(electrical$value &lt; security$value) * 100</code></pre>
<pre><code>## [1] 81.145</code></pre>
</div>
<div id="wrapping-up" class="section level2">
<h2>Wrapping up</h2>
<p>With data simulation we can account for sample size, uncertainty and interpretability. We can achieve this understanding without building something entirely new either.</p>
<p>This methodology can be applied to any aspect of digital marketing where data is the lifeblood of the channel. For example…</p>
<ul>
<li>Rankings: your daily rank went from #9 to #1 only to celebrate too early and it drops back down to #8 the next day. If you calculated the probability of hitting #1 from the source data, would you not have celebrated as early?</li>
<li>Traffic: was the spike in site visitors a real phenomenon or just random chance?</li>
<li>Click-through Rate: how do you handle low volume data where you only received 2 clicks and 2 impressions? You don’t want to kick out data because it is telling you something! (check out my R guide and the section on <a href="https://www.christopheryee.org/blog/getting-started-with-r-using-google-search-console-data/">estimating CTR with empirical Bayes</a>)</li>
</ul>
</div>
<div id="intentional-exclusion" class="section level2">
<h2>Intentional exclusion</h2>
<p>I specifically left out certain concepts to get the reader excited about using R to simulate their own data. Although important, I will leave those for future articles but the following are notes for myself:</p>
<ul>
<li>Foundation for Bayesian statistics</li>
<li>No mention of incorporating conjugate priors</li>
<li>Other R distribution functions: <strong>dnorm</strong>, <strong>pnorm</strong>, <strong>qnorm</strong></li>
<li>Minimized mathematical notation</li>
</ul>
</div>
